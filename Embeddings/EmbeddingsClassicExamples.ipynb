{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Classic Examples\n",
    "This short notebook shows the classic examples used to demonstrate the math of embeddings, \n",
    "\n",
    "- vector[Queen] =  vector[King]  - vector[Man] + vector[Woman]\n",
    "\n",
    "- vector[Madrid] = vector[Spain] - vector[Italy] + vector[Rome]\n",
    "\n",
    "We first import the gensim library and load the word2vec embeddings. Then we compute, \n",
    "\n",
    "- vector[King]  - vector[Man] + vector[Woman]\n",
    "- vector[Spain] - vector[Italy] + vector[Rome]\n",
    "\n",
    "and look for the words with the most similar vectors. Note that in both cases the most similar one is the starting work (King and Spain) the expected result (Queen and Madrid) appear as second word.\n",
    " \n",
    "Similarity is computed using Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import fasttext\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec300 = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec300.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300518155097961),\n",
       " ('monarch', 0.645466148853302),\n",
       " ('princess', 0.6156251430511475),\n",
       " ('crown_prince', 0.5818676948547363),\n",
       " ('prince', 0.5777117609977722),\n",
       " ('kings', 0.5613664388656616),\n",
       " ('sultan', 0.5376776456832886),\n",
       " ('Queen_Consort', 0.5344247221946716),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.array(word2vec300['king'])-np.array(word2vec300['man'])+np.array(word2vec300['woman'])\n",
    "word2vec300.most_similar(positive=[e], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Madrid', 0.6931530237197876),\n",
       " ('Seville', 0.5892879366874695),\n",
       " ('Barcelona', 0.5854125618934631),\n",
       " ('Athens', 0.51669842004776),\n",
       " ('Seville_Spain', 0.5157975554466248),\n",
       " ('Giles_Tremlett', 0.5132015347480774),\n",
       " ('Santiago_de_Compostela', 0.5131174325942993),\n",
       " ('Valencia', 0.5016350150108337),\n",
       " ('Spaniards', 0.4990643262863159),\n",
       " ('Paris', 0.48526036739349365)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec300.most_similar(positive=[\"Spain\",\"Rome\"], negative=[\"Italy\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rome', 0.7016042470932007),\n",
       " ('Madrid', 0.6859280467033386),\n",
       " ('Spain', 0.599949836730957),\n",
       " ('Seville', 0.5847654938697815),\n",
       " ('Barcelona', 0.576607882976532),\n",
       " ('Athens', 0.5255098938941956),\n",
       " ('Seville_Spain', 0.5133261680603027),\n",
       " ('Santiago_de_Compostela', 0.5105167627334595),\n",
       " ('Giles_Tremlett', 0.5102741718292236),\n",
       " ('Paris', 0.4927102327346802)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.array(word2vec300['Spain'])-np.array(word2vec300['Italy'])+np.array(word2vec300['Rome'])\n",
    "word2vec300.most_similar(positive=[e], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gynecologist', 0.7093892097473145),\n",
       " ('nurse', 0.647728681564331),\n",
       " ('doctors', 0.6471461653709412),\n",
       " ('physician', 0.6438996195793152),\n",
       " ('pediatrician', 0.6249487996101379),\n",
       " ('nurse_practitioner', 0.6218312978744507),\n",
       " ('obstetrician', 0.6072015166282654),\n",
       " ('ob_gyn', 0.5986713171005249),\n",
       " ('midwife', 0.5927063226699829),\n",
       " ('dermatologist', 0.5739566683769226)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec300.most_similar(positive=[\"doctor\",\"woman\"], negative=[\"man\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doctor', 0.883492112159729),\n",
       " ('gynecologist', 0.7276508212089539),\n",
       " ('nurse', 0.6698511838912964),\n",
       " ('physician', 0.6674119830131531),\n",
       " ('doctors', 0.664949357509613),\n",
       " ('pediatrician', 0.6398378014564514),\n",
       " ('nurse_practitioner', 0.6237460374832153),\n",
       " ('obstetrician', 0.6188927292823792),\n",
       " ('midwife', 0.6041982769966125),\n",
       " ('dentist', 0.5999662280082703)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.array(word2vec300['doctor'])-np.array(word2vec300['man'])+np.array(word2vec300['woman'])\n",
    "word2vec300.most_similar(positive=[e], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same example using fasttext\n",
    "To use the example you must download the English word embeddings in the fasttex directory from,\n",
    "\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "or using the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to download\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_facebook_model(\"./fasttext/cc.en.300.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7554813623428345),\n",
       " ('queen-mother', 0.6141631603240967),\n",
       " ('princess', 0.5755329728126526),\n",
       " ('monarch', 0.5741075277328491),\n",
       " ('kings', 0.5688967704772949),\n",
       " ('queenship', 0.5649926066398621),\n",
       " ('Queen', 0.5638619661331177),\n",
       " ('empress', 0.5544731020927429),\n",
       " ('consort', 0.5524798035621643),\n",
       " ('queen.The', 0.5497488379478455)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.wv.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.7286674380302429),\n",
       " ('queen', 0.6542678475379944),\n",
       " ('kings', 0.5410280823707581),\n",
       " ('queen-mother', 0.5250692367553711),\n",
       " ('Queen', 0.5074419975280762),\n",
       " ('royal', 0.500452995300293),\n",
       " ('king-', 0.4945007264614105),\n",
       " ('queens', 0.49149513244628906),\n",
       " ('monarch', 0.4913707971572876),\n",
       " ('queenship', 0.48369985818862915)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.array(ft.wv['king'])-np.array(ft.wv['man'])+np.array(ft.wv['woman'])\n",
    "ft.wv.most_similar(positive=[e], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
