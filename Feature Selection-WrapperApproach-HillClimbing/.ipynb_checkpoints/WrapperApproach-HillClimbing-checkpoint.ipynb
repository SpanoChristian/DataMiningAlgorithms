{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper Approach - Hill Climbing\n",
    "In this notebook we implement a rather simple feature selection procedure that follows a wrapper approach. The search algorithm, hill climbing in this case, is wrapped around the target classification/regression algorithm.\n",
    "\n",
    "First we import the libraries that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the data and generate the k-fold evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_boston()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data[\"data\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "\n",
    "number_of_variables = X.shape[1]\n",
    "input_variables = data.feature_names\n",
    "target_variable = 'MEDV'\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "# let's create also a pandas data frame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['MEDV'] = y\n",
    "df.head()\n",
    "\n",
    "kfolds = KFold(10,shuffle=True,random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying a wrapper approach we are searching for the best subset of feature for a target model. In this case we will search for the best subset of features for plain linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateFeatureSubsetSingleObjective(individual):\n",
    "    selected_columns = []\n",
    "    for i,allele in enumerate(individual):\n",
    "        if (allele==1):\n",
    "            selected_columns.append(df.columns[i])\n",
    "\n",
    "    model = linear_model.LinearRegression()\n",
    "    scores = cross_val_score(model, df[selected_columns], y, cv=kfolds)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HillClimbing(number_of_variables,number_of_evaluations,evaluation_function):\n",
    "\n",
    "    # current evaluation\n",
    "    evaluations = 0\n",
    "    \n",
    "    # start from a random set of features\n",
    "    current_feature_subset = [random.randint(0,1) for x in range(number_of_variables)]\n",
    "\n",
    "    # that will also provide an initial evaluation of the best performance\n",
    "    best_performance = evaluation_function(current_feature_subset)\n",
    "    \n",
    "    print(\"%5d\\t\\t%3.2f\\t%s\"%(evaluations,best_performance,str(current_feature_subset)))\n",
    "    \n",
    "    # continue until all the evaluations have been performed\n",
    "    while evaluations<number_of_evaluations:\n",
    "        \n",
    "        # generate a neighbor candidate using a 10% perturbation of the current subset\n",
    "        perturbation = [(lambda x: 1-x if (random.random()<0.1) else x)(x) for x in current_feature_subset]\n",
    "\n",
    "        # evaluate only if there is at least one variable\n",
    "        if (sum(perturbation)>0):\n",
    "            performance = evaluation_function(perturbation)\n",
    "\n",
    "            if (performance>best_performance):\n",
    "                best_performance = performance\n",
    "                current_feature_subset = perturbation\n",
    "\n",
    "        evaluations = evaluations + 1\n",
    "        print(\"%5d\\t\\t%3.2f\\t%s\"%(evaluations,best_performance,str(current_feature_subset)))\n",
    "\n",
    "    print(\"Best Feature Subset = %s \"%(str(current_feature_subset)))\n",
    "    print(\"Performance = %3.2f\"%(best_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run hill-climbing for 100 evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\t\t0.26\t[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "    1\t\t0.26\t[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "    2\t\t0.26\t[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "    3\t\t0.26\t[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "    4\t\t0.26\t[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "    5\t\t0.27\t[0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
      "    6\t\t0.52\t[1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "    7\t\t0.53\t[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "    8\t\t0.60\t[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
      "    9\t\t0.67\t[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n",
      "   10\t\t0.67\t[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n",
      "   11\t\t0.67\t[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n",
      "   12\t\t0.67\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "   13\t\t0.67\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "   14\t\t0.67\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "   15\t\t0.67\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "   16\t\t0.67\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "   17\t\t0.67\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "   18\t\t0.67\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "   19\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
      "   20\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
      "   21\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   22\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   23\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   24\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   25\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   26\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   27\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   28\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   29\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   30\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   31\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   32\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   33\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   34\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   35\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   36\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   37\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   38\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   39\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   40\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   41\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   42\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   43\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   44\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   45\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   46\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   47\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   48\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   49\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   50\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   51\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   52\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   53\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "   54\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   55\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   56\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   57\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   58\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   59\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   60\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   61\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   62\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   63\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   64\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   65\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   66\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   67\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   68\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   69\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   70\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   71\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   72\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   73\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   74\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   75\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   76\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   77\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   78\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   79\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   80\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   81\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   82\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   83\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   84\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   85\t\t0.68\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "   86\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   87\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   88\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   89\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   90\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   91\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   92\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   93\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   94\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   95\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   96\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   97\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   98\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "   99\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "  100\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Best Feature Subset = [1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1] \n",
      "Performance = 0.69\n"
     ]
    }
   ],
   "source": [
    "HillClimbing(number_of_variables,100,EvaluateFeatureSubsetSingleObjective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same process targeting another model like for instance a k-nearest-neighbour regressor with a k of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateFeatureSubsetKNN(individual):\n",
    "    selected_columns = []\n",
    "    for i,allele in enumerate(individual):\n",
    "        if (allele==1):\n",
    "            selected_columns.append(df.columns[i])\n",
    "\n",
    "    model = KNeighborsRegressor(5)\n",
    "    scores = cross_val_score(model, df[selected_columns], y, cv=kfolds)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\t\t0.31\t[0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
      "    1\t\t0.32\t[0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
      "    2\t\t0.32\t[0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
      "    3\t\t0.32\t[0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
      "    4\t\t0.54\t[0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
      "    5\t\t0.54\t[0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
      "    6\t\t0.54\t[0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
      "    7\t\t0.56\t[0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "    8\t\t0.56\t[0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "    9\t\t0.56\t[0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   10\t\t0.57\t[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   11\t\t0.57\t[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   12\t\t0.57\t[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   13\t\t0.57\t[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   14\t\t0.57\t[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   15\t\t0.57\t[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   16\t\t0.59\t[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   17\t\t0.59\t[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   18\t\t0.59\t[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   19\t\t0.59\t[1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "   20\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   21\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   22\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   23\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   24\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   25\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   26\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   27\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   28\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   29\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   30\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   31\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   32\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   33\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   34\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   35\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   36\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   37\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   38\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   39\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   40\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   41\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   42\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   43\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   44\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   45\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   46\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   47\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   48\t\t0.60\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "   49\t\t0.63\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
      "   50\t\t0.63\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
      "   51\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   52\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   53\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   54\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   55\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   56\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   57\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   58\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   59\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   60\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   61\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   62\t\t0.67\t[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   63\t\t0.67\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "   64\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   65\t\t0.69\t[1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   66\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   67\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   68\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   69\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   70\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   71\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   72\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   73\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   74\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   75\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   76\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   77\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   78\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   79\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   80\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   81\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   82\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   83\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   84\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   85\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   86\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   87\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   88\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   89\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   90\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   91\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   92\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   93\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   94\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   95\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   96\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   97\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   98\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "   99\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "  100\t\t0.73\t[1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "Best Feature Subset = [1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1] \n",
      "Performance = 0.73\n"
     ]
    }
   ],
   "source": [
    "HillClimbing(number_of_variables,100,EvaluateFeatureSubsetKNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Note that, with k-NN, we were able to reach a better performance with much fewer features. Might we draw some insight from this result? Also note that when doing feature selection we used the entire dataset but feature selection is, as a matter of fact, similar to the search of the hyper-parameter alpha for Lasso/Ridge regression so we shou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
